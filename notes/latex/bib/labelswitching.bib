% This file was created with JabRef 2.5.
% Encoding: ISO8859_1

@ARTICLE{Barger:2008,
  author = {Kathryn Barger and John Bunge},
  title = {Bayesian estimation of the number of species using noninformative
	priors.},
  journal = {Biom J},
  year = {2008},
  volume = {50},
  pages = {1064--1076},
  number = {6},
  month = {Dec},
  __markedentry = {[goshng]},
  abstract = {Consider a sample of animal abundances collected from one sampling
	occasion. Our focus is in estimating the number of species in a closed
	population. In order to conduct a noninformative Bayesian inference
	when modeling this data, we derive Jeffreys and reference priors
	from the full likelihood. We assume that the species' abundances
	are randomly distributed according to a distribution indexed by a
	finite-dimensional parameter. We consider two specific cases which
	assume that the mean abundances are constant or exponentially distributed.
	The Jeffreys and reference priors are functions of the Fisher information
	for the model parameters; the information is calculated in part using
	the linear difference score for integer parameter models (Lindsay
	& Roeder 1987). The Jeffreys and reference priors perform similarly
	in a data example we consider. The posteriors based on the Jeffreys
	and reference priors are proper.},
  doi = {10.1002/bimj.200810445},
  institution = {Department of Statistical Science, Cornell University, Ithaca, NY
	14853, USA. kjb34@cornell.edu},
  keywords = {Bayes Theorem; Biodiversity; Computer Simulation; Models, Biological;
	Population Density; Water Microbiology; cal},
  language = {eng},
  medline-pst = {ppublish},
  owner = {goshng},
  pmid = {19035547},
  review = {An interesting ... one ... not HW},
  timestamp = {2009.11.02},
  url = {http://dx.doi.org/10.1002/bimj.200810445}
}

@INCOLLECTION{Celeux:1998,
  author = {Celeux, G.},
  title = {Beyesian inference for mixtures: The label­switching problem},
  booktitle = {COMPSTAT 98},
  publisher = {Physica­Verlag},
  year = {1998},
  editor = {R. Payne and P.Green},
  pages = {227--232},
  owner = {goshng},
  timestamp = {2009.11.02}
}

@ARTICLE{Celeux:1997,
  author = {Celeux, G.},
  title = {Contribution to the discussion of paper by Richardson and Green (1997)},
  journal = {Journal of the Royal Statistical Society, series B},
  year = {1997},
  volume = {59},
  pages = {775--776},
  owner = {goshng},
  timestamp = {2009.11.02}
}

@ARTICLE{Celeux:2000,
  author = {Celeux, G. And Hurn, M. And Robert, C.P.},
  title = {Computational and inferential difficulties with  
	
	mixture posterior distributions. },
  journal = {Available from the MCMC Preprint service},
  year = {2000},
  owner = {goshng},
  timestamp = {2009.11.02}
}

@ARTICLE{Fraley:2007,
  author = {Fraley, Chris and Raftery, Adrian E.},
  title = {Bayesian Regularization for Normal Mixture Estimation and Model-Based
	Clustering},
  journal = {Journal of Classification},
  year = {2007},
  volume = {24},
  pages = {155--181},
  number = {2},
  abstract = {Abstract\&nbsp;\&nbsp;Normal mixture models are widely used for statistical
	modeling of data, including cluster analysis. However maximum likelihood
	estimation (MLE) for normal mixtures using the EM algorithm may fail
	as the result of singularities or degeneracies. To avoid this, we
	propose replacing the MLE by a maximum a posteriori (MAP) estimator,
	also found by the EM algorithm. For choosing the number of components
	and the model parameterization, we propose a modified version of
	BIC, where the likelihood is evaluated at the MAP instead of the
	MLE. We use a highly dispersed proper conjugate prior, containing
	a small fraction of one observation's worth of information. The resulting
	method avoids degeneracies and singularities, but when these are
	not present it gives similar results to the standard method using
	MLE, EM and BIC.},
  citeulike-article-id = {2667079},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/s00357-007-0004-5},
  doi = {10.1007/s00357-007-0004-5},
  keywords = {bayesian, classification, clustering, em, fitting, gaussian, mclust,
	mixture, normal, r},
  posted-at = {2009-06-11 14:14:45},
  priority = {2},
  url = {http://dx.doi.org/10.1007/s00357-007-0004-5}
}

@ARTICLE{Fraley:2002,
  author = {Fraley, C. and Raftery, A. E.},
  title = {Model-based clustering, discriminant analysis, and density estimation},
  journal = {J. Am. Stat. Assoc.},
  year = {2002},
  volume = {97},
  pages = {611--631},
  number = {458},
  abstract = {Cluster analysis is the automated search for groups of related observations
	in a dataset. Most clustering done in practice is based largely on
	heuristic but intuitively reasonable procedures, and most clustering
	methods available in commercial software are also of this type. However,
	there is little systematic guidance associated with these methods
	for solving important practical questions that arise in cluster analysis,
	such as how many clusters are there, which clustering method should
	be used, and how should outliers be handled. We review a general
	methodology for model-based clustering that provides a principled
	statistical approach to these issues. We also show that this can
	be useful for other problems in multivariate analysis, such as discriminant
	analysis and multivariate density estimation. We give examples from
	medical diagnosis, minefield detection, cluster recovery from noisy
	data, and spatial density estimation. Finally, we mention limitations
	of the methodology and discuss recent developments in model-based
	clustering for non-Gaussian data, high-dimensional datasets, large
	datasets, and Bayesian estimation.},
  address = {Department of Statistics, University of Washington, Box 354322, Seattle,
	WA 98195, United States},
  citeulike-article-id = {2109572},
  citeulike-linkout-0 = {http://dx.doi.org/10.1198/016214502760047131},
  citeulike-linkout-1 = {http://www.scopus.com/record/display.url?view=extended&origin=resultslist&eid=2-s2.0-0035998835},
  comment = {initialization via model-based hierarchical agglomerative clustering,
	maximum likelihood estimation via expectation-maximization (EM) algorithm,
	and the selection of the model and the number of components using
	approximate Bayes factors with the Bayesian information criterion
	(BIC) approximation},
  doi = {10.1198/016214502760047131},
  keywords = {bic, cluster, density\_estimation, discriminant\_analysis, lib\_statistics,
	mclust, model-based, spatial},
  posted-at = {2007-12-14 01:24:14},
  priority = {2},
  url = {http://dx.doi.org/10.1198/016214502760047131}
}

@BOOK{Fruhwith:2006,
  title = {Finite Mixture and Markov Switching Models},
  publisher = {Springer Series in Statistics},
  year = {2006},
  author = {Fruhwirth­Schnatter, Sylvai},
  owner = {goshng},
  timestamp = {2009.11.02}
}

@ARTICLE{Fruhwirth:2001,
  author = {Fruhwirth­Schnatter, Sylvia},
  title = {MCMC Estimation of Classical and Dynamic Switching and Mixture Models},
  journal = { Journal of the American Statistical Association},
  year = {2001},
  volume = {96},
  pages = {194--209},
  owner = {goshng},
  timestamp = {2009.11.02}
}

@ARTICLE{Brn:2009,
  author = {Grn, Bettina and Leisch, Friedrich},
  title = {Dealing with label switching in mixture models under genuine multimodality},
  journal = {Journal of Multivariate Analysis},
  year = {2009},
  volume = {100},
  pages = {851-861},
  number = {5},
  month = {May},
  abstract = {The fitting of finite mixture models is an ill-defined estimation
	problem, as completely different parameterizations can induce similar
	mixture distributions. This leads to multiple modes in the likelihood,
	which is a problem for frequentist maximum likelihood estimation,
	and complicates statistical inference of Markov chain Monte Carlo
	draws in Bayesian estimation. For the analysis of the posterior density
	of these draws, a suitable separation into different modes is desirable.
	In addition, a unique labelling of the component specific estimates
	is necessary to solve the label switching problem. This paper presents
	and compares two approaches to achieve these goals: relabelling under
	multimodality and constrained clustering. The algorithmic details
	are discussed, and their application is demonstrated on artificial
	and real-world data.},
  keywords = { 62H30 62F15 Constrained clustering Finite mixture models Label switching
	Multimodality},
  url = {http://ideas.repec.org/a/eee/jmvana/v100y2009i5p851-861.html}
}

@ARTICLE{Huelsenbeck:2007,
  author = {John P Huelsenbeck and Peter Andolfatto},
  title = {Inference of population structure under a Dirichlet process model.},
  journal = {Genetics},
  year = {2007},
  volume = {175},
  pages = {1787--1802},
  number = {4},
  month = {Apr},
  abstract = {Inferring population structure from genetic data sampled from some
	number of individuals is a formidable statistical problem. One widely
	used approach considers the number of populations to be fixed and
	calculates the posterior probability of assigning individuals to
	each population. More recently, the assignment of individuals to
	populations and the number of populations have both been considered
	random variables that follow a Dirichlet process prior. We examined
	the statistical behavior of assignment of individuals to populations
	under a Dirichlet process prior. First, we examined a best-case scenario,
	in which all of the assumptions of the Dirichlet process prior were
	satisfied, by generating data under a Dirichlet process prior. Second,
	we examined the performance of the method when the genetic data were
	generated under a population genetics model with symmetric migration
	between populations. We examined the accuracy of population assignment
	using a distance on partitions. The method can be quite accurate
	with a moderate number of loci. As expected, inferences on the number
	of populations are more accurate when theta = 4N(e)u is large and
	when the migration rate (4N(e)m) is low. We also examined the sensitivity
	of inferences of population structure to choice of the parameter
	of the Dirichlet process model. Although inferences could be sensitive
	to the choice of the prior on the number of populations, this sensitivity
	occurred when the number of loci sampled was small; inferences are
	more robust to the prior on the number of populations when the number
	of sampled loci is large. Finally, we discuss several methods for
	summarizing the results of a Bayesian Markov chain Monte Carlo (MCMC)
	analysis of population structure. We develop the notion of the mean
	population partition, which is the partition of individuals to populations
	that minimizes the squared partition distance to the partitions sampled
	by the MCMC algorithm.},
  doi = {10.1534/genetics.106.061317},
  institution = {Department of Integrative Biology, University of California, Berkeley,
	California 94720, USA. johnh@berkeley.edu},
  keywords = {Algorithms; Alleles; Animals; Bayes Theorem; Computer Simulation;
	Gene Frequency; Genetics, Population; Markov Chains; Mice; Models,
	Genetic; Models, Statistical; Monte Carlo Method; Ruminants; Songbirds},
  language = {eng},
  medline-pst = {ppublish},
  owner = {goshng},
  pii = {genetics.106.061317},
  pmid = {17237522},
  timestamp = {2009.10.29},
  url = {http://dx.doi.org/10.1534/genetics.106.061317}
}

@ARTICLE{Jain:2004,
  author = {Jain, Sonia and Neal, Radford M.},
  title = {A Split-Merge Markov Chain Monte Carlo Procedure for the Dirichlet
	Process Mixture Model},
  journal = {Journal of Computational \& Graphical Statistics},
  year = {2004},
  volume = {13},
  pages = {158--182},
  number = {1},
  month = {March},
  abstract = {This article proposes a split-merge Markov chain algorithm to address
	the problem of inefficient sampling for conjugate Dirichlet process
	mixture models. Traditional Markov chain Monte Carlo methods for
	Bayesian mixture models, such as Gibbs sampling, can become trapped
	in isolated modes corresponding to an inappropriate clustering of
	data points. This article describes a Metropolis-Hastings procedure
	that can escape such local modes by splitting or merging mixture
	components. Our algorithm employs a new technique in which an appropriate
	proposal for splitting or merging components is obtained by using
	a restricted Gibbs sampling scan. We demonstrate empirically that
	our method outperforms the Gibbs sampler in situations where two
	or more components are similar in structure.},
  citeulike-article-id = {1318117},
  citeulike-linkout-0 = {http://dx.doi.org/10.1198/1061860043001},
  citeulike-linkout-1 = {http://www.ingentaconnect.com/content/asa/jcgs/2004/00000013/00000001/art00010},
  citeulike-linkout-2 = {http://www.utstat.toronto.edu/~sonia/paper1.pdf},
  doi = {10.1198/1061860043001},
  keywords = {carlo, chain, dirichlet, markov, mixture, model, monte, process, split-merge},
  posted-at = {2008-10-22 13:50:04},
  priority = {2},
  url = {http://dx.doi.org/10.1198/1061860043001}
}

@ARTICLE{Jakobsson:2007,
  author = {Mattias Jakobsson and Noah A Rosenberg},
  title = {CLUMPP: a cluster matching and permutation program for dealing with
	label switching and multimodality in analysis of population structure.},
  journal = {Bioinformatics},
  year = {2007},
  volume = {23},
  pages = {1801--1806},
  number = {14},
  month = {Jul},
  abstract = {MOTIVATION: Clustering of individuals into populations on the basis
	of multilocus genotypes is informative in a variety of settings.
	In population-genetic clustering algorithms, such as BAPS, STRUCTURE
	and TESS, individual multilocus genotypes are partitioned over a
	set of clusters, often using unsupervised approaches that involve
	stochastic simulation. As a result, replicate cluster analyses of
	the same data may produce several distinct solutions for estimated
	cluster membership coefficients, even though the same initial conditions
	were used. Major differences among clustering solutions have two
	main sources: (1) 'label switching' of clusters across replicates,
	caused by the arbitrary way in which clusters in an unsupervised
	analysis are labeled, and (2) 'genuine multimodality,' truly distinct
	solutions across replicates. RESULTS: To facilitate the interpretation
	of population-genetic clustering results, we describe three algorithms
	for aligning multiple replicate analyses of the same data set. We
	have implemented these algorithms in the computer program CLUMPP
	(CLUster Matching and Permutation Program). We illustrate the use
	of CLUMPP by aligning the cluster membership coefficients from 100
	replicate cluster analyses of 600 chickens from 20 different breeds.
	AVAILABILITY: CLUMPP is freely available at http://rosenberglab.bioinformatics.med.umich.edu/clumpp.html.},
  doi = {10.1093/bioinformatics/btm233},
  institution = {Center for Computational Medicine and Biology, Department of Human
	Genetics, University of Michigan, Ann Arbor, MI, USA. mjakob@umich.edu},
  keywords = {Algorithms; Animals; Chickens; Cluster Analysis; Computational Biology;
	Data Interpretation, Statistical; Genetics, Population; Genotype;
	Models, Genetic; Models, Statistical; Population Dynamics; Software},
  owner = {goshng},
  pii = {btm233},
  pmid = {17485429},
  timestamp = {2008.10.17},
  url = {http://dx.doi.org/10.1093/bioinformatics/btm233}
}

@ARTICLE{Jasra:2005,
  author = {A. Jasra and C.C. Holmes and D.A. Stephens},
  title = {Markov Chain Monte Carlo Methods and the Label Switching Problem in Bayesian Mixture Modelling},
  year = {2005},
  owner = {goshng},
  timestamp = {2009.11.02},
  url = {http://www.stats.ox.ac.uk/~cholmes/Reports/mcmc_label_switching_holmes.pdf}
}

@ARTICLE{Kim:2006,
  author = {Sinae Kim and Mahlet G. Tadesse and Marina Vannucci},
  title = {Variable selection in clustering via Dirichlet process mixture models},
  journal = {Biometrika},
  year = {2006},
  volume = {93},
  pages = {877-893},
  number = {4},
  month = {December},
  abstract = { The increased collection of high-dimensional data in various fields
	has raised a strong interest in clustering algorithms and variable
	selection procedures. In this paper, we propose a model-based method
	that addresses the two problems simultaneously. We introduce a latent
	binary vector to identify discriminating variables and use Dirichlet
	process mixture models to define the cluster structure. We update
	the variable selection index using a Metropolis algorithm and obtain
	inference on the cluster structure via a split-merge Markov chain
	Monte Carlo technique. We explore the performance of the methodology
	on simulated data and illustrate an application with a DNA microarray
	study. Copyright 2006, Oxford University Press.},
  url = {http://ideas.repec.org/a/oup/biomet/v93y2006i4p877-893.html}
}

@ARTICLE{Lartillot:2004,
  author = {Nicolas Lartillot and Hervé Philippe},
  title = {A Bayesian mixture model for across-site heterogeneities in the amino-acid
	replacement process.},
  journal = {Mol Biol Evol},
  year = {2004},
  volume = {21},
  pages = {1095--1109},
  number = {6},
  month = {Jun},
  abstract = {Most current models of sequence evolution assume that all sites of
	a protein evolve under the same substitution process, characterized
	by a 20 x 20 substitution matrix. Here, we propose to relax this
	assumption by developing a Bayesian mixture model that allows the
	amino-acid replacement pattern at different sites of a protein alignment
	to be described by distinct substitution processes. Our model, named
	CAT, assumes the existence of distinct processes (or classes) differing
	by their equilibrium frequencies over the 20 residues. Through the
	use of a Dirichlet process prior, the total number of classes and
	their respective amino-acid profiles, as well as the affiliations
	of each site to a given class, are all free variables of the model.
	In this way, the CAT model is able to adapt to the complexity actually
	present in the data, and it yields an estimate of the substitutional
	heterogeneity through the posterior mean number of classes. We show
	that a significant level of heterogeneity is present in the substitution
	patterns of proteins, and that the standard one-matrix model fails
	to account for this heterogeneity. By evaluating the Bayes factor,
	we demonstrate that the standard model is outperformed by CAT on
	all of the data sets which we analyzed. Altogether, these results
	suggest that the complexity of the pattern of substitution of real
	sequences is better captured by the CAT model, offering the possibility
	of studying its impact on phylogenetic reconstruction and its connections
	with structure-function determinants.},
  doi = {10.1093/molbev/msh112},
  institution = {Canadian Institute for Advanced Research, Département de Biochimie,
	Université de Montréal, Montréal, Québec Canada. nicolas.lartillot@lirmm.fr},
  keywords = {Amino Acid Sequence; Amino Acid Substitution, genetics; Bayes Theorem;
	Evolution, Molecular; Models, Genetic; Phylogeny; Sequence Alignment},
  language = {eng},
  medline-pst = {ppublish},
  owner = {goshng},
  pii = {msh112},
  pmid = {15014145},
  timestamp = {2009.11.02},
  url = {http://dx.doi.org/10.1093/molbev/msh112}
}

@ELECTRONIC{Lopes:2008,
  author = {Lopes, H},
  year = {2008},
  title = {Multivariate Mixture of Normals},
  url = {http://faculty.chicagobooth.edu/hedibert.lopes/teaching/41913­Winter2008/},
  owner = {goshng},
  timestamp = {2009.11.02}
}

@ARTICLE{marin:2005,
  author = {Marin, M/ J. and Mengerson, K. and Robert, C. P.},
  title = {{Bayesian Modelling and Inference on Mixtures of Distributions}},
  journal = {Handbook of Statistics},
  year = {2009},
  pages = {15840--15845},
  citeulike-article-id = {3463263},
  keywords = {bayesian, heterogeneous, mixture, model, non-homogeneous},
  posted-at = {2008-11-08 01:26:32},
  priority = {2}
}

@ARTICLE{Meligkotsidou:2007,
  author = {Loukia Meligkotsidou},
  title = {Bayesian multivariate Poisson mixtures with an unknown number of
	components},
  journal = {Statistics and Computing},
  year = {2007},
  volume = {17},
  pages = {93--107},
  owner = {goshng},
  timestamp = {2009.10.29}
}

@ARTICLE{Neal:2000,
  author = {Neal, Radford M.},
  title = {Markov Chain Sampling Methods for Dirichlet Process Mixture Models},
  journal = {Journal of Computational and Graphical Statistics},
  year = {2000},
  volume = {9},
  pages = {249--265},
  number = {2},
  abstract = {This article reviews Markov chain methods for sampling from the posterior
	distribution of a Dirichlet process mixture model and presents two
	new classes of methods. One new approach is to make Metropolis-Hastings
	updates of the indicators specifying which mixture component is associated
	with each observation, perhaps supplemented with a partial form of
	Gibbs sampling. The other new approach extends Gibbs sampling for
	these indicators by using a set of auxiliary parameters. These methods
	are simple to implement and are more efficient than previous ways
	of handling general Dirichlet process mixture models with non-conjugate
	priors.},
  citeulike-article-id = {634913},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/1390653},
  citeulike-linkout-1 = {http://www.jstor.org/stable/1390653},
  doi = {10.2307/1390653},
  issn = {10618600},
  keywords = {bayesian, chain, dirichlet, markov, process, sampling},
  posted-at = {2007-10-04 17:59:11},
  priority = {0},
  publisher = {American Statistical Association, Institute of Mathematical Statistics,
	and Interface Foundation of America},
  url = {http://dx.doi.org/10.2307/1390653}
}

@ARTICLE{Nobile:2007,
  author = {Agostino Nobile and Alastair T. Fearnside},
  title = {Bayesian finite mixtures with an unknown number of components: The
	allocation sampler},
  journal = {Statistics and Computing},
  year = {2007},
  volume = {17},
  pages = {147--162},
  owner = {goshng},
  timestamp = {2009.10.29}
}

@ARTICLE{Peltonen:2009,
  author = {Jaakko Peltonen and Jarkko Venna and Samuel Kaski},
  title = {Visualizations for assessing convergence and mixing of Markov chain
	Monte Carlo simulations},
  journal = {Computational Statistics \& Data Analysis},
  year = {2009},
  volume = {53},
  pages = {4453 - 4470},
  number = {12},
  abstract = {Bayesian inference often requires approximating the posterior distribution
	by Markov chain Monte Carlo sampling. The samples come from the true
	distribution only after the simulation has converged, which makes
	detecting convergence a central problem. Commonly, several simulation
	chains are started from different points, and their overlap is used
	as a measure of convergence. Convergence measures cannot tell the
	analyst the cause of convergence problems; it is suggested that complementing
	them with proper visualization will help. A novel connection is pointed
	out: linear discriminant analysis (LDA) minimizes the overlap of
	the simulation chains measured by a common multivariate convergence
	measure. LDA is thus justified for visualizing convergence. However,
	LDA makes restrictive assumptions about the chains, which can be
	relaxed by a recent extension called discriminative component analysis
	(DCA). Lastly, methods are introduced for unidentifiable models and
	model families with variable number of parameters, where straightforward
	visualization in the parameter space is not feasible.},
  doi = {DOI: 10.1016/j.csda.2009.07.001},
  issn = {0167-9473},
  url = {http://www.sciencedirect.com/science/article/B6V8V-4WPJ5X4-1/2/c64d26755300cf365ccd167112d9836f}
}

@ARTICLE{Rasmussen:1999,
  author = {Rasmussen, C.},
  title = {The countably infinite bayesian gaussian mixture density model},
  year = {1999},
  citeulike-article-id = {843523},
  citeulike-linkout-0 = {http://citeseer.ist.psu.edu/rasmussen00infinite.html},
  citeulike-linkout-1 = {http://citeseer.lcs.mit.edu/rasmussen00infinite.html},
  citeulike-linkout-2 = {http://citeseer.ifi.unizh.ch/rasmussen00infinite.html},
  citeulike-linkout-3 = {http://citeseer.comp.nus.edu.sg/rasmussen00infinite.html},
  keywords = {mixture-modelling},
  posted-at = {2006-09-14 17:41:05},
  priority = {2},
  review = {Search the Internet.}
}

@ARTICLE{Rasmussen:2000,
  author = {Rasmussen, Carl E.},
  title = {The infinite Gaussian mixture model},
  year = {2000},
  volume = {12},
  pages = {554--560},
  abstract = {In a Bayesian mixture model it is not necessary a priori to limit
	the number of components to be finite. In this paper an infinite
	Gaussian mixture model is presented which neatly sidesteps the difficult
	problem of finding the \&amp;quot;right \&amp;quot; number of mixture
	components. Inference in the model is done using an efficient parameter-free
	Markov Chain that relies entirely on Gibbs sampling. 1},
  booktitle = {In Advances in Neural Information Processing Systems 12},
  citeulike-article-id = {3478609},
  citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.9111},
  keywords = {infinite\_mixturemodels},
  posted-at = {2008-11-04 14:27:00},
  priority = {2},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.9111}
}

@ELECTRONIC{Rosenthal:2009,
  author = {Rosenthal, J.},
  year = {2009},
  title = {Notes on MCMC.},
  url = {http://www.probability.ca/jeff/teaching/},
  owner = {goshng},
  timestamp = {2009.11.02}
}

@ARTICLE{Scott:2002,
  author = {S. L. Scott},
  title = {Bayesian Methods for Hidden Markov Models: Recursive Computing in
	the 21st Century},
  journal = {Journal of the American Statistical Association},
  year = {2002},
  volume = {97},
  pages = {337-351},
  month = {March},
  url = {http://ideas.repec.org/a/bes/jnlasa/v97y2002mmarchp337-351.html}
}

@ARTICLE{Sperrin:2009,
  author = {Sperrin, M. and Jaki, T. and Wit, E.},
  title = {Probabilistic relabelling strategies for the label switching problem
	in Bayesian mixture models},
  journal = {Statistics and Computing},
  abstract = {Abstract\&nbsp;\&nbsp;The label switching problem is caused by the
	likelihood of a Bayesian mixture model being invariant to permutations
	of the labels. The permutation can change multiple times between
	Markov Chain Monte Carlo (MCMC) iterations making it difficult to
	infer component-specific parameters of the model. Various so-called
	'relabelling' strategies exist with the goal to 'undo' the label
	switches that have occurred to enable estimation of functions that
	depend on component-specific parameters. Existing deterministic relabelling
	algorithms rely upon specifying a loss function, and relabelling
	by minimising its posterior expected loss. In this paper we develop
	probabilistic approaches to relabelling that allow for estimation
	and incorporation of the uncertainty in the relabelling process.
	Variants of the probabilistic relabelling algorithm are introduced
	and compared to existing deterministic relabelling algorithms. We
	demonstrate that the idea of probabilistic relabelling can be expressed
	in a rigorous framework based on the EM algorithm.},
  citeulike-article-id = {4500010},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/s11222-009-9129-8},
  citeulike-linkout-1 = {http://www.springerlink.com/content/6730162354305785},
  doi = {10.1007/s11222-009-9129-8},
  posted-at = {2009-05-10 12:03:36},
  url = {http://dx.doi.org/10.1007/s11222-009-9129-8}
}

@ARTICLE{Stephens:2000,
  author = {Stephens, M.},
  title = {Dealing with label-switching in mixture models.},
  journal = {J. R. Stat. Soc. Ser. B},
  year = {2000},
  volume = {62},
  pages = {795--809},
  citeulike-article-id = {5298765},
  keywords = {label-switching, mixture, model},
  owner = {goshng},
  posted-at = {2009-07-29 14:51:49},
  priority = {2},
  timestamp = {2009.10.29}
}

@ARTICLE{Stephens:1997,
  author = {Stephens, M.},
  title = {Contribution to the discussion of paper by Richardson and Green (1997)},
  journal = {Journal of the Royal Statistics Society, series B},
  year = {1997},
  volume = {59},
  pages = {768--769},
  owner = {goshng},
  timestamp = {2009.11.02}
}

@ARTICLE{Stephens:1997a,
  author = {Stephens, M.},
  title = {Bayesian Methods for Mixtures of Normal Distributions.},
  journal = {Ph.D. Thesis, University of Oxford.},
  year = {1997},
  owner = {goshng},
  timestamp = {2009.11.02},
  url = {http://www.stats.ox.ac.uk/~stepens}
}

@ARTICLE{Wang:2007,
  author = {Liqun Wang and James C. Fu},
  title = {A practical sampling approach for a Bayesian mixture model with unknown
	number of components},
  journal = {Statistical Papers},
  year = {2007},
  volume = {48},
  pages = {631--653},
  owner = {goshng},
  review = {It claims that it is an efficient method.},
  timestamp = {2009.10.29}
}

@TECHREPORT{Xie:2009,
  author = {Xie, Fanfu and Chen, ZhengFei},
  title = {Label Switch in Mixture Model and Relabeling Algorithm},
  institution = {Project for Reading Course},
  year = {2009},
  file = {:Users/goshng/Documents/Papers/MCMCRelabelProject.pdf:PDF},
  owner = {goshng},
  timestamp = {2009.11.02},
  url = {http://probability.ca/jeff/grad.html}
}

@ARTICLE{Yao:2009,
  author = {Yao, Weixin and Lindsay, Bruce G.},
  title = {Bayesian Mixture Labeling by Highest Posterior Density},
  journal = {Journal of the American Statistical Association},
  year = {2009},
  volume = {104},
  pages = {758-767},
  number = {486},
  abstract = { A fundamental problem for Bayesian mixture model analysis is label
	switching, which occurs as a result of the nonidentifiability of
	the mixture components under symmetric priors. We propose two labeling
	methods to solve this problem. The first method, denoted by PM(ALG),
	is based on the posterior modes and an ascending algorithm generically
	denoted ALG. We use each Markov chain Monte Carlo sample as the starting
	point in an ascending algorithm, and label the sample based on the
	mode of the posterior to which it converges. Our natural assumption
	here is that the samples converged to the same mode should have the
	same labels. The PM(ALG) labeling method has some computational advantages
	over other popular labeling methods. Additionally, it automatically
	matches the ideal labels in the highest posterior density credible
	regions. The second method does labeling by maximizing the normal
	likelihood of the labeled Gibbs samples. Using a Monte Carlo simulation
	study and a real dataset, we demonstrate the success of our new methods
	in dealing with the label switching problem. },
  doi = {10.1198/jasa.2009.0237},
  eprint = {http://pubs.amstat.org/doi/pdf/10.1198/jasa.2009.0237},
  url = {http://pubs.amstat.org/doi/abs/10.1198/jasa.2009.0237}
}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

